{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import KeyedVectors\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(-1)\n",
    "\n",
    "torch.cuda.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, idim = 300, odim = 4, activation = ['Tanh'], dropout = [0], gpu = 0, nhid = []):\n",
    "        \n",
    "        super(FeedNet, self).__init__()\n",
    "        \n",
    "        num_layers = len(nhid)\n",
    "        num_activations = len(activation)\n",
    "        num_dropout = len(dropout)\n",
    "        \n",
    "        mul_act = False\n",
    "        mul_drop = False\n",
    "        if num_activations > 1:\n",
    "            mul_act = True\n",
    "        if num_dropout > 1:\n",
    "            mul_drop = True\n",
    "            \n",
    "        if ((mul_act or mul_drop) and\n",
    "            (num_activations != num_layers+1 or num_dropout!= num_layers+1)):\n",
    "            raise Exception(\"Number of activation functions and dropouts, should be 1 or it should be equal to number of layers\")\n",
    "        \n",
    "        modules = []\n",
    "        print('-Network {:d}'.format(idim), end='')\n",
    "        if num_layers > 0 :\n",
    "                n_prev = idim\n",
    "                for i, n in enumerate(nhid):\n",
    "                                                \n",
    "                    if mul_drop and dropout[i] > 0:\n",
    "                        print('-{:d}drop'.format(dropout[i]), end='')\n",
    "                        modules.append(nn.Dropout(p=dropout[i]))\n",
    "                    else:\n",
    "                        if dropout[0]>0:\n",
    "                            modules.append(nn.Dropout(p=dropout[0]))\n",
    "                \n",
    "                    modules.append(nn.Linear(n_prev, n))\n",
    "                    n_prev = n\n",
    "                    if mul_act:\n",
    "                        if activation[i] == 'Tanh':\n",
    "                            print('-{:d}t'.format(n), end='')\n",
    "                            modules.append(nn.Tanh())\n",
    "                        elif activation[i] == 'Relu':\n",
    "                            print('-{:d}r'.format(n), end='')\n",
    "                            modules.append(nn.ReLU())\n",
    "                        else:\n",
    "                            print('Unrecognizable activation Function, the defualt activation is replace.')\n",
    "                            print('-{:d}t'.format(n), end='')\n",
    "                            modules.append(nn.Tanh())\n",
    "                    else:\n",
    "                        if activation[0] == 'Tanh':\n",
    "                            print('-{:d}t'.format(n), end='')\n",
    "                            modules.append(nn.Tanh())\n",
    "                        elif activation[0] == 'Relu':\n",
    "                            print('-{:d}r'.format(n), end='')\n",
    "                            modules.append(nn.ReLU())\n",
    "                        else:\n",
    "                            print('-{:d}t'.format(n), end='')\n",
    "                            print('Unrecognizable activation Function, the defualt activation is replace.')\n",
    "                            modules.append(nn.Tanh())\n",
    "                            \n",
    "                modules.append(nn.Linear(n_prev,odim))            \n",
    "        else:\n",
    "             modules.append(nn.Linear(idim, odim))\n",
    "        if mul_drop:\n",
    "            print('-{:d}'.format(odim))\n",
    "        else:\n",
    "            print('-{:d}, dropout={:.1f}'.format(odim, dropout[0]))\n",
    "                \n",
    "        self.net = nn.Sequential(*modules)\n",
    "        if gpu:\n",
    "            self.net = self.net.cuda()\n",
    "            \n",
    "            \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    \n",
    "    def evaluate(self, dataset, name= 'Dev', nlables = '4'):\n",
    "        num_correct = 0\n",
    "        num_sample = 0\n",
    "        self.net.train(mode=False)\n",
    "        with torch.no_grad():\n",
    "            for data in dataset:\n",
    "                feature, label = data\n",
    "                label = label.long()\n",
    "                label = label.squeeze()\n",
    "                output = self.net(feature)\n",
    "                _, predictions = torch.max(output, 1)\n",
    "                num_correct += (predictions == label).int().sum()\n",
    "                num_sample += label.shape[0]\n",
    "            \n",
    "            acc = 100.0 * (num_correct.float()/num_sample)\n",
    "            print(' | {:s}-accuracy : {:4f}'.format(name, acc), end='')\n",
    "        return num_correct, acc\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class textData(Dataset):\n",
    "    \n",
    "    def __init__(self, docs_path, model, dim, prefix = 'en_', language = 'english', subwords = False):\n",
    "        \n",
    "        print(\"Loading the dataset\")\n",
    "        dic = {'CCAT':0, 'ECAT':1, 'MCAT':2, 'GCAT':3}\n",
    "        df = pd.read_csv(docs_path, sep='\\t')\n",
    "        df['category'] = [dic[item] for item in df.label]\n",
    "        df = self.tokenized(df)\n",
    "        self.number_of_samples  = len(df)\n",
    "\n",
    "        print(\"Generating Document Embeddings\")\n",
    "        if not subwords:\n",
    "            d_emb = self.doc_emb(df, model, prefix,language, dim)\n",
    "            df['embedding'] = d_emb\n",
    "        else:\n",
    "            d_emb = self.doc_emb_subword(df,model, prefix,language, dim)\n",
    "            df['embedding'] = d_emb\n",
    "        \n",
    "        self.x = torch.zeros([len(df),dim])\n",
    "        for i,emb in enumerate(df['embedding']):\n",
    "            self.x[i] = torch.from_numpy(emb)\n",
    "        \n",
    "        self.y = torch.zeros([len(df),1])\n",
    "        for i,cat in enumerate(df['category']):\n",
    "            self.y[i] = cat\n",
    "        \n",
    "           \n",
    "        \n",
    "    def __len__(self):\n",
    "        #len(dataset)\n",
    "        return self.number_of_samples\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        #for indexing dataset[0]\n",
    "        #return self.x[index], self.y[index]\n",
    "        sample = self.x[index], self.y[index]\n",
    "        return sample\n",
    "    \n",
    "    def tokenized(self, df):\n",
    "        n=0\n",
    "        courp= []\n",
    "        for item in df.iterrows():\n",
    "            src = item[1]['script']\n",
    "            src = src.lower()\n",
    "            tokens = word_tokenize(src)\n",
    "            #tokens = MicroTokenizer.cut_by_joint_model(src)\n",
    "            #tokens = japtoken(src)\n",
    "            words = [ word for word in tokens if word.isalpha()]\n",
    "            courp.append(words)\n",
    "        df['tokenized'] = courp\n",
    "        return df\n",
    "\n",
    "    def doc_emb(self, df, model, lan,language, dim):\n",
    "        document_emb = []\n",
    "        not_in = 0\n",
    "        in_words = 0\n",
    "        progress_bar = tqdm(df, leave=False)\n",
    "        for i, script in enumerate(df['tokenized']):\n",
    "                final_emb = np.zeros((1,dim))\n",
    "                counter = 0\n",
    "                for word in script:\n",
    "                    if word in stopwords.words(language):\n",
    "                        continue\n",
    "                    m_word = lan + word\n",
    "                    if m_word in model:\n",
    "                        vector = model[m_word]\n",
    "                        norm_v = vector/np.linalg.norm(vector)\n",
    "                        final_emb = np.add(norm_v, final_emb)\n",
    "                        counter += 1 \n",
    "                        in_words += 1\n",
    "                    else:\n",
    "                        not_in += 1\n",
    "                final_emb = np.divide(final_emb, counter)\n",
    "                norm_finalemb = final_emb / np.linalg.norm(final_emb)\n",
    "                document_emb.append(norm_finalemb)\n",
    "                progress_bar.set_description(\"Processing \")\n",
    "                progress_bar.update(1)\n",
    "        return document_emb\n",
    "    \n",
    "    def doc_emb_subword(self, df, model, lan,language, dim):\n",
    "        document_emb = []\n",
    "        for i, script in enumerate(df['tokenized']):\n",
    "                final_emb = np.zeros((1,dim))\n",
    "                counter = 0\n",
    "                for word in script:\n",
    "\n",
    "                    if word in stopwords.words(language):\n",
    "                        continue\n",
    "\n",
    "                    m_word = lan + word\n",
    "                    subwords = GetSubWords1(m_word)\n",
    "                    vector = np.zeros((1,dim), dtype='float')\n",
    "                    c = 0;\n",
    "                    for subs in subwords:\n",
    "                        if subs in model:\n",
    "                            c += 1\n",
    "                            embedding = model[subs]\n",
    "                            vector = np.sum([vector,embedding], axis=0, keepdims=True)[0]\n",
    "                    if np.all(vector==0):      \n",
    "                            continue\n",
    "                    vector = np.divide(vector, c)\n",
    "                    norm_v = vector/np.linalg.norm(vector)\n",
    "                    final_emb = np.add(norm_v, final_emb)\n",
    "                    counter += 1 \n",
    "\n",
    "                final_emb = np.divide(final_emb, counter)\n",
    "\n",
    "                norm_finalemb = final_emb / np.linalg.norm(final_emb)\n",
    "                document_emb.append(norm_finalemb)\n",
    "        return document_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here We initialize the network and do the training loop\n",
    "### The train, dev and test dataset should be prepared beforehand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_model = KeyedVectors.load_word2vec_format(\"/Users/cons13411/xlingualembedding evaluation/Doung/en-it/en.it.combin.ruters.outputn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataset\n",
      "Generating Document Embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1cb0c179645499a910162e63b78bd19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataset\n",
      "Generating Document Embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99ea9435e3c34d6eadc9ff572c0f8b54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_set = textData('/Users/cons13411/Downloads/RCV!/mldoc/en.train.1000.txt',emb_model,\n",
    "                    dim=200, subwords=False)\n",
    "train_loader = DataLoader(train_set, batch_size=12, shuffle=False)\n",
    "dev_set = textData('/Users/cons13411/Downloads/RCV!/mldoc/en.dev.txt', emb_model,\n",
    "                    dim=200, subwords=False)\n",
    "dev_loader = DataLoader(dev_set, batch_size=12, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-Network 200-10t-4, dropout=0.2\n",
      "Ep    0 | loss 110.188841 | Train_Accuracy:41.900000 | Dev-accuracy : 62.400002| saved\n",
      "Ep    1 | loss 90.067120 | Train_Accuracy:76.400000 | Dev-accuracy : 79.699997| saved\n",
      "Ep    2 | loss 71.720166 | Train_Accuracy:78.900000 | Dev-accuracy : 81.900002| saved\n",
      "Ep    3 | loss 57.918761 | Train_Accuracy:83.400000 | Dev-accuracy : 86.500000| saved\n",
      "Ep    4 | loss 47.809055 | Train_Accuracy:87.100000 | Dev-accuracy : 88.900002| saved\n",
      "Ep    5 | loss 41.164240 | Train_Accuracy:86.200000 | Dev-accuracy : 90.300003| saved\n",
      "Ep    6 | loss 36.033276 | Train_Accuracy:88.700000 | Dev-accuracy : 91.299995| saved\n",
      "Ep    7 | loss 31.566147 | Train_Accuracy:89.600000 | Dev-accuracy : 91.699997| saved\n",
      "Ep    8 | loss 30.092519 | Train_Accuracy:88.900000 | Dev-accuracy : 91.699997\n",
      "Ep    9 | loss 27.481609 | Train_Accuracy:89.000000 | Dev-accuracy : 91.799995| saved\n",
      "Ep   10 | loss 25.503176 | Train_Accuracy:90.500000 | Dev-accuracy : 91.900002| saved\n",
      "Ep   11 | loss 24.693869 | Train_Accuracy:90.500000 | Dev-accuracy : 92.400002| saved\n",
      "Ep   12 | loss 23.769845 | Train_Accuracy:90.400000 | Dev-accuracy : 92.299995\n",
      "Ep   13 | loss 22.637937 | Train_Accuracy:91.000000 | Dev-accuracy : 92.500000| saved\n",
      "Ep   14 | loss 21.762708 | Train_Accuracy:91.100000 | Dev-accuracy : 92.699997| saved\n",
      "Ep   15 | loss 19.822290 | Train_Accuracy:92.100000 | Dev-accuracy : 92.400002\n",
      "Ep   16 | loss 19.716687 | Train_Accuracy:91.700000 | Dev-accuracy : 92.400002\n",
      "Ep   17 | loss 19.846302 | Train_Accuracy:91.700000 | Dev-accuracy : 92.500000\n",
      "Ep   18 | loss 19.840249 | Train_Accuracy:91.800000 | Dev-accuracy : 92.699997\n",
      "Ep   19 | loss 17.694398 | Train_Accuracy:93.300000 | Dev-accuracy : 92.799995| saved\n",
      "Ep   20 | loss 18.021232 | Train_Accuracy:92.600000 | Dev-accuracy : 93.000000| saved\n",
      "Ep   21 | loss 17.973909 | Train_Accuracy:92.100000 | Dev-accuracy : 93.000000\n",
      "Ep   22 | loss 18.236460 | Train_Accuracy:92.400000 | Dev-accuracy : 93.300003| saved\n",
      "Ep   23 | loss 17.055676 | Train_Accuracy:92.700000 | Dev-accuracy : 93.000000\n",
      "Ep   24 | loss 16.927681 | Train_Accuracy:93.100000 | Dev-accuracy : 93.199997\n",
      "Ep   25 | loss 16.104582 | Train_Accuracy:93.200000 | Dev-accuracy : 93.099998\n",
      "Ep   26 | loss 16.346490 | Train_Accuracy:92.400000 | Dev-accuracy : 93.199997\n",
      "Ep   27 | loss 16.098347 | Train_Accuracy:92.900000 | Dev-accuracy : 93.199997\n",
      "Ep   28 | loss 16.166629 | Train_Accuracy:93.200000 | Dev-accuracy : 92.900002\n",
      "Ep   29 | loss 15.545113 | Train_Accuracy:92.900000 | Dev-accuracy : 93.000000\n",
      "Ep   30 | loss 15.917281 | Train_Accuracy:92.500000 | Dev-accuracy : 92.900002\n",
      "Ep   31 | loss 15.242721 | Train_Accuracy:93.100000 | Dev-accuracy : 92.599998\n",
      "Ep   32 | loss 15.094465 | Train_Accuracy:93.600000 | Dev-accuracy : 92.699997\n",
      "Ep   33 | loss 14.282764 | Train_Accuracy:93.800000 | Dev-accuracy : 92.500000\n",
      "Ep   34 | loss 14.435119 | Train_Accuracy:93.700000 | Dev-accuracy : 93.000000\n",
      "Ep   35 | loss 14.329295 | Train_Accuracy:93.800000 | Dev-accuracy : 92.500000\n",
      "Ep   36 | loss 14.297653 | Train_Accuracy:93.500000 | Dev-accuracy : 92.900002\n",
      "Ep   37 | loss 14.758602 | Train_Accuracy:93.500000 | Dev-accuracy : 92.599998\n",
      "Ep   38 | loss 13.963558 | Train_Accuracy:93.700000 | Dev-accuracy : 92.500000\n",
      "Ep   39 | loss 14.586356 | Train_Accuracy:92.900000 | Dev-accuracy : 92.599998\n",
      "Ep   40 | loss 14.071520 | Train_Accuracy:94.500000 | Dev-accuracy : 92.299995\n",
      "Ep   41 | loss 13.830352 | Train_Accuracy:94.100000 | Dev-accuracy : 92.400002\n",
      "Ep   42 | loss 13.641090 | Train_Accuracy:94.400000 | Dev-accuracy : 92.400002\n",
      "Ep   43 | loss 13.560073 | Train_Accuracy:93.000000 | Dev-accuracy : 92.199997\n",
      "Ep   44 | loss 12.746387 | Train_Accuracy:94.200000 | Dev-accuracy : 92.099998\n",
      "Ep   45 | loss 12.013791 | Train_Accuracy:94.800000 | Dev-accuracy : 92.500000\n",
      "Ep   46 | loss 12.903460 | Train_Accuracy:93.800000 | Dev-accuracy : 92.400002\n",
      "Ep   47 | loss 13.667553 | Train_Accuracy:93.300000 | Dev-accuracy : 92.900002\n",
      "Ep   48 | loss 13.007783 | Train_Accuracy:94.400000 | Dev-accuracy : 92.900002\n",
      "Ep   49 | loss 13.381754 | Train_Accuracy:94.700000 | Dev-accuracy : 92.299995\n",
      "Ep   50 | loss 13.410241 | Train_Accuracy:93.600000 | Dev-accuracy : 92.900002\n",
      "Ep   51 | loss 13.813752 | Train_Accuracy:93.100000 | Dev-accuracy : 92.599998\n",
      "Ep   52 | loss 12.109792 | Train_Accuracy:94.800000 | Dev-accuracy : 92.699997\n",
      "Ep   53 | loss 13.561468 | Train_Accuracy:94.700000 | Dev-accuracy : 92.199997\n",
      "Ep   54 | loss 13.981705 | Train_Accuracy:93.500000 | Dev-accuracy : 92.299995\n",
      "Ep   55 | loss 13.482692 | Train_Accuracy:93.700000 | Dev-accuracy : 92.299995\n",
      "Ep   56 | loss 12.220150 | Train_Accuracy:94.400000 | Dev-accuracy : 92.099998\n",
      "Ep   57 | loss 12.153994 | Train_Accuracy:95.000000 | Dev-accuracy : 92.299995\n",
      "Ep   58 | loss 12.940683 | Train_Accuracy:93.500000 | Dev-accuracy : 92.000000\n",
      "Ep   59 | loss 12.340829 | Train_Accuracy:94.300000 | Dev-accuracy : 92.400002\n",
      "Ep   60 | loss 12.100618 | Train_Accuracy:94.700000 | Dev-accuracy : 92.400002\n",
      "Ep   61 | loss 12.149309 | Train_Accuracy:94.200000 | Dev-accuracy : 92.699997\n",
      "Ep   62 | loss 12.104186 | Train_Accuracy:95.500000 | Dev-accuracy : 92.599998\n",
      "Ep   63 | loss 11.064314 | Train_Accuracy:94.500000 | Dev-accuracy : 92.299995\n",
      "Ep   64 | loss 11.364087 | Train_Accuracy:95.400000 | Dev-accuracy : 92.400002\n",
      "Ep   65 | loss 10.691682 | Train_Accuracy:95.600000 | Dev-accuracy : 92.400002\n",
      "Ep   66 | loss 11.751842 | Train_Accuracy:95.500000 | Dev-accuracy : 91.900002\n",
      "Ep   67 | loss 11.345531 | Train_Accuracy:94.800000 | Dev-accuracy : 92.199997\n",
      "Ep   68 | loss 11.714398 | Train_Accuracy:94.200000 | Dev-accuracy : 92.599998\n",
      "Ep   69 | loss 10.841582 | Train_Accuracy:95.100000 | Dev-accuracy : 92.799995\n",
      "Ep   70 | loss 11.870225 | Train_Accuracy:94.800000 | Dev-accuracy : 92.900002\n",
      "Ep   71 | loss 11.783892 | Train_Accuracy:94.800000 | Dev-accuracy : 92.500000\n",
      "Ep   72 | loss 11.367003 | Train_Accuracy:94.300000 | Dev-accuracy : 92.799995\n",
      "Ep   73 | loss 11.588438 | Train_Accuracy:94.700000 | Dev-accuracy : 92.900002\n",
      "Ep   74 | loss 11.156431 | Train_Accuracy:95.600000 | Dev-accuracy : 92.900002\n",
      "Ep   75 | loss 10.634956 | Train_Accuracy:94.800000 | Dev-accuracy : 93.000000\n",
      "Ep   76 | loss 11.322697 | Train_Accuracy:94.700000 | Dev-accuracy : 92.599998\n",
      "Ep   77 | loss 10.221158 | Train_Accuracy:95.300000 | Dev-accuracy : 92.500000\n",
      "Ep   78 | loss 10.152000 | Train_Accuracy:95.500000 | Dev-accuracy : 92.699997\n",
      "Ep   79 | loss 10.405592 | Train_Accuracy:94.800000 | Dev-accuracy : 92.400002\n",
      "Ep   80 | loss 10.925452 | Train_Accuracy:95.000000 | Dev-accuracy : 92.299995\n",
      "Ep   81 | loss 11.111548 | Train_Accuracy:95.200000 | Dev-accuracy : 92.599998\n",
      "Ep   82 | loss 10.625211 | Train_Accuracy:95.400000 | Dev-accuracy : 92.599998\n",
      "Ep   83 | loss 10.858095 | Train_Accuracy:95.200000 | Dev-accuracy : 92.500000\n",
      "Ep   84 | loss 10.755602 | Train_Accuracy:95.800000 | Dev-accuracy : 92.299995\n",
      "Ep   85 | loss 11.147171 | Train_Accuracy:94.900000 | Dev-accuracy : 92.500000\n",
      "Ep   86 | loss 11.108959 | Train_Accuracy:95.000000 | Dev-accuracy : 92.199997\n",
      "Ep   87 | loss 11.101125 | Train_Accuracy:95.100000 | Dev-accuracy : 92.400002\n",
      "Ep   88 | loss 9.092572 | Train_Accuracy:96.100000 | Dev-accuracy : 91.799995\n",
      "Ep   89 | loss 9.733506 | Train_Accuracy:95.900000 | Dev-accuracy : 91.900002\n",
      "Ep   90 | loss 10.500140 | Train_Accuracy:95.500000 | Dev-accuracy : 92.099998\n",
      "Ep   91 | loss 11.999172 | Train_Accuracy:94.600000 | Dev-accuracy : 91.799995\n",
      "Ep   92 | loss 9.919518 | Train_Accuracy:95.500000 | Dev-accuracy : 92.199997\n",
      "Ep   93 | loss 9.914742 | Train_Accuracy:95.300000 | Dev-accuracy : 92.199997\n",
      "Ep   94 | loss 11.837948 | Train_Accuracy:94.300000 | Dev-accuracy : 92.099998\n",
      "Ep   95 | loss 10.931600 | Train_Accuracy:95.100000 | Dev-accuracy : 92.400002\n",
      "Ep   96 | loss 10.525911 | Train_Accuracy:94.900000 | Dev-accuracy : 92.599998\n",
      "Ep   97 | loss 10.465544 | Train_Accuracy:94.900000 | Dev-accuracy : 92.299995\n",
      "Ep   98 | loss 11.054932 | Train_Accuracy:94.200000 | Dev-accuracy : 92.299995\n",
      "Ep   99 | loss 11.377623 | Train_Accuracy:94.600000 | Dev-accuracy : 92.199997\n",
      "Ep  100 | loss 11.514385 | Train_Accuracy:94.300000 | Dev-accuracy : 92.299995\n",
      "Ep  101 | loss 10.040528 | Train_Accuracy:94.900000 | Dev-accuracy : 91.900002\n",
      "Ep  102 | loss 11.053399 | Train_Accuracy:95.500000 | Dev-accuracy : 91.799995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep  103 | loss 10.186024 | Train_Accuracy:95.500000 | Dev-accuracy : 92.299995\n",
      "Ep  104 | loss 10.259380 | Train_Accuracy:94.700000 | Dev-accuracy : 92.099998\n",
      "Ep  105 | loss 10.451052 | Train_Accuracy:95.500000 | Dev-accuracy : 92.900002\n",
      "Ep  106 | loss 11.093861 | Train_Accuracy:95.400000 | Dev-accuracy : 92.199997\n",
      "Ep  107 | loss 11.434726 | Train_Accuracy:94.300000 | Dev-accuracy : 92.199997\n",
      "Ep  108 | loss 8.500238 | Train_Accuracy:95.800000 | Dev-accuracy : 92.299995\n",
      "Ep  109 | loss 10.810313 | Train_Accuracy:95.100000 | Dev-accuracy : 92.000000\n",
      "Ep  110 | loss 9.934686 | Train_Accuracy:95.400000 | Dev-accuracy : 92.199997\n",
      "Ep  111 | loss 8.786099 | Train_Accuracy:95.100000 | Dev-accuracy : 91.900002\n",
      "Ep  112 | loss 9.551738 | Train_Accuracy:96.000000 | Dev-accuracy : 92.099998\n",
      "Ep  113 | loss 9.838837 | Train_Accuracy:95.400000 | Dev-accuracy : 91.900002\n",
      "Ep  114 | loss 9.781544 | Train_Accuracy:95.200000 | Dev-accuracy : 92.099998\n",
      "Ep  115 | loss 7.599399 | Train_Accuracy:97.300000 | Dev-accuracy : 92.500000\n",
      "Ep  116 | loss 11.714770 | Train_Accuracy:94.100000 | Dev-accuracy : 92.199997\n",
      "Ep  117 | loss 10.430434 | Train_Accuracy:95.400000 | Dev-accuracy : 93.000000\n",
      "Ep  118 | loss 10.757000 | Train_Accuracy:94.800000 | Dev-accuracy : 92.299995\n",
      "Ep  119 | loss 9.192285 | Train_Accuracy:96.100000 | Dev-accuracy : 92.599998\n",
      "Ep  120 | loss 9.721044 | Train_Accuracy:95.600000 | Dev-accuracy : 92.900002\n",
      "Ep  121 | loss 10.124141 | Train_Accuracy:95.800000 | Dev-accuracy : 92.400002\n",
      "Ep  122 | loss 10.634328 | Train_Accuracy:94.800000 | Dev-accuracy : 92.699997\n",
      "Ep  123 | loss 8.979883 | Train_Accuracy:96.300000 | Dev-accuracy : 92.099998\n",
      "Ep  124 | loss 9.490462 | Train_Accuracy:96.200000 | Dev-accuracy : 92.599998\n",
      "Ep  125 | loss 10.226492 | Train_Accuracy:95.600000 | Dev-accuracy : 92.500000\n",
      "Ep  126 | loss 9.393631 | Train_Accuracy:95.000000 | Dev-accuracy : 92.699997\n",
      "Ep  127 | loss 9.620747 | Train_Accuracy:95.100000 | Dev-accuracy : 92.000000\n",
      "Ep  128 | loss 10.190228 | Train_Accuracy:95.300000 | Dev-accuracy : 92.599998\n",
      "Ep  129 | loss 9.335557 | Train_Accuracy:95.600000 | Dev-accuracy : 92.500000\n",
      "Ep  130 | loss 8.812726 | Train_Accuracy:96.000000 | Dev-accuracy : 92.000000\n",
      "Ep  131 | loss 10.198616 | Train_Accuracy:94.900000 | Dev-accuracy : 92.400002\n",
      "Ep  132 | loss 9.144829 | Train_Accuracy:95.600000 | Dev-accuracy : 92.400002\n",
      "Ep  133 | loss 10.803907 | Train_Accuracy:94.800000 | Dev-accuracy : 92.099998\n",
      "Ep  134 | loss 10.005221 | Train_Accuracy:95.700000 | Dev-accuracy : 92.299995\n",
      "Ep  135 | loss 9.522377 | Train_Accuracy:95.200000 | Dev-accuracy : 92.299995\n",
      "Ep  136 | loss 11.507335 | Train_Accuracy:94.600000 | Dev-accuracy : 91.900002\n",
      "Ep  137 | loss 9.582898 | Train_Accuracy:95.200000 | Dev-accuracy : 92.099998\n",
      "Ep  138 | loss 10.694518 | Train_Accuracy:94.200000 | Dev-accuracy : 91.900002\n",
      "Ep  139 | loss 9.635187 | Train_Accuracy:96.000000 | Dev-accuracy : 92.400002\n",
      "Ep  140 | loss 10.921230 | Train_Accuracy:94.700000 | Dev-accuracy : 92.400002\n",
      "Ep  141 | loss 9.038290 | Train_Accuracy:95.100000 | Dev-accuracy : 92.299995\n",
      "Ep  142 | loss 10.499755 | Train_Accuracy:94.900000 | Dev-accuracy : 92.299995\n",
      "Ep  143 | loss 10.012164 | Train_Accuracy:95.100000 | Dev-accuracy : 92.000000\n",
      "Ep  144 | loss 9.691372 | Train_Accuracy:96.400000 | Dev-accuracy : 92.500000\n",
      "Ep  145 | loss 9.584984 | Train_Accuracy:94.800000 | Dev-accuracy : 92.400002\n",
      "Ep  146 | loss 9.760844 | Train_Accuracy:95.300000 | Dev-accuracy : 92.500000\n",
      "Ep  147 | loss 10.487111 | Train_Accuracy:94.900000 | Dev-accuracy : 92.599998\n",
      "Ep  148 | loss 9.068951 | Train_Accuracy:95.300000 | Dev-accuracy : 92.400002\n",
      "Ep  149 | loss 9.916063 | Train_Accuracy:95.300000 | Dev-accuracy : 92.299995\n",
      "Ep  150 | loss 8.966870 | Train_Accuracy:95.500000 | Dev-accuracy : 92.699997\n",
      "Ep  151 | loss 10.101533 | Train_Accuracy:95.400000 | Dev-accuracy : 91.699997\n",
      "Ep  152 | loss 10.271316 | Train_Accuracy:95.400000 | Dev-accuracy : 92.299995\n",
      "Ep  153 | loss 11.095919 | Train_Accuracy:93.800000 | Dev-accuracy : 92.099998\n",
      "Ep  154 | loss 10.314167 | Train_Accuracy:95.000000 | Dev-accuracy : 92.299995\n",
      "Ep  155 | loss 8.723317 | Train_Accuracy:95.900000 | Dev-accuracy : 92.400002\n",
      "Ep  156 | loss 9.462467 | Train_Accuracy:95.700000 | Dev-accuracy : 92.500000\n",
      "Ep  157 | loss 8.451467 | Train_Accuracy:96.200000 | Dev-accuracy : 92.699997\n",
      "Ep  158 | loss 8.491533 | Train_Accuracy:96.600000 | Dev-accuracy : 93.000000\n",
      "Ep  159 | loss 8.865939 | Train_Accuracy:96.300000 | Dev-accuracy : 92.699997\n",
      "Ep  160 | loss 9.341897 | Train_Accuracy:95.700000 | Dev-accuracy : 92.900002\n",
      "Ep  161 | loss 9.288478 | Train_Accuracy:96.300000 | Dev-accuracy : 92.199997\n",
      "Ep  162 | loss 8.350027 | Train_Accuracy:96.500000 | Dev-accuracy : 92.400002\n",
      "Ep  163 | loss 8.979242 | Train_Accuracy:95.300000 | Dev-accuracy : 92.000000\n",
      "Ep  164 | loss 8.560927 | Train_Accuracy:95.900000 | Dev-accuracy : 92.799995\n",
      "Ep  165 | loss 8.517232 | Train_Accuracy:96.000000 | Dev-accuracy : 93.000000\n",
      "Ep  166 | loss 9.372631 | Train_Accuracy:95.700000 | Dev-accuracy : 92.699997\n",
      "Ep  167 | loss 9.909176 | Train_Accuracy:95.700000 | Dev-accuracy : 92.199997\n",
      "Ep  168 | loss 8.572013 | Train_Accuracy:97.000000 | Dev-accuracy : 92.699997\n",
      "Ep  169 | loss 9.193584 | Train_Accuracy:95.700000 | Dev-accuracy : 92.299995\n",
      "Ep  170 | loss 9.288430 | Train_Accuracy:95.600000 | Dev-accuracy : 92.000000\n",
      "Ep  171 | loss 7.733109 | Train_Accuracy:96.700000 | Dev-accuracy : 93.000000\n",
      "Ep  172 | loss 10.094517 | Train_Accuracy:95.000000 | Dev-accuracy : 92.400002\n",
      "Ep  173 | loss 8.755000 | Train_Accuracy:96.500000 | Dev-accuracy : 92.099998\n",
      "Ep  174 | loss 9.455743 | Train_Accuracy:95.700000 | Dev-accuracy : 92.400002\n",
      "Ep  175 | loss 8.077370 | Train_Accuracy:96.600000 | Dev-accuracy : 91.599998\n",
      "Ep  176 | loss 8.854176 | Train_Accuracy:95.700000 | Dev-accuracy : 92.400002\n",
      "Ep  177 | loss 9.278814 | Train_Accuracy:95.900000 | Dev-accuracy : 91.900002\n",
      "Ep  178 | loss 9.055492 | Train_Accuracy:95.800000 | Dev-accuracy : 92.299995\n",
      "Ep  179 | loss 9.371909 | Train_Accuracy:95.900000 | Dev-accuracy : 92.099998\n",
      "Ep  180 | loss 8.123717 | Train_Accuracy:95.500000 | Dev-accuracy : 91.900002\n",
      "Ep  181 | loss 8.238466 | Train_Accuracy:96.200000 | Dev-accuracy : 92.199997\n",
      "Ep  182 | loss 8.192865 | Train_Accuracy:96.100000 | Dev-accuracy : 91.799995\n",
      "Ep  183 | loss 10.215783 | Train_Accuracy:95.200000 | Dev-accuracy : 91.900002\n",
      "Ep  184 | loss 9.272071 | Train_Accuracy:96.000000 | Dev-accuracy : 92.400002\n",
      "Ep  185 | loss 9.347103 | Train_Accuracy:95.300000 | Dev-accuracy : 92.299995\n",
      "Ep  186 | loss 7.895081 | Train_Accuracy:96.400000 | Dev-accuracy : 92.199997\n",
      "Ep  187 | loss 8.096111 | Train_Accuracy:96.200000 | Dev-accuracy : 92.199997\n",
      "Ep  188 | loss 7.449407 | Train_Accuracy:96.800000 | Dev-accuracy : 92.099998\n",
      "Ep  189 | loss 7.680752 | Train_Accuracy:96.400000 | Dev-accuracy : 92.199997\n",
      "Ep  190 | loss 8.208322 | Train_Accuracy:96.500000 | Dev-accuracy : 92.099998\n",
      "Ep  191 | loss 10.529067 | Train_Accuracy:94.600000 | Dev-accuracy : 91.799995\n",
      "Ep  192 | loss 9.309377 | Train_Accuracy:96.100000 | Dev-accuracy : 92.400002\n",
      "Ep  193 | loss 9.059065 | Train_Accuracy:95.800000 | Dev-accuracy : 91.900002\n",
      "Ep  194 | loss 9.284700 | Train_Accuracy:95.700000 | Dev-accuracy : 92.299995\n",
      "Ep  195 | loss 10.645508 | Train_Accuracy:94.000000 | Dev-accuracy : 92.400002\n",
      "Ep  196 | loss 8.733079 | Train_Accuracy:95.300000 | Dev-accuracy : 92.099998\n",
      "Ep  197 | loss 10.057597 | Train_Accuracy:95.300000 | Dev-accuracy : 91.799995\n",
      "Ep  198 | loss 8.259733 | Train_Accuracy:96.000000 | Dev-accuracy : 92.000000\n",
      "Ep  199 | loss 10.244303 | Train_Accuracy:94.400000 | Dev-accuracy : 91.699997\n",
      "Ep  200 | loss 9.367993 | Train_Accuracy:96.100000 | Dev-accuracy : 92.500000\n",
      "Ep  201 | loss 9.724125 | Train_Accuracy:95.500000 | Dev-accuracy : 92.199997\n",
      "Ep  202 | loss 8.812109 | Train_Accuracy:95.600000 | Dev-accuracy : 91.799995\n",
      "Ep  203 | loss 7.951056 | Train_Accuracy:96.200000 | Dev-accuracy : 92.400002\n",
      "Ep  204 | loss 6.686776 | Train_Accuracy:97.600000 | Dev-accuracy : 91.900002\n",
      "Ep  205 | loss 8.858844 | Train_Accuracy:96.000000 | Dev-accuracy : 92.400002\n",
      "Ep  206 | loss 9.361056 | Train_Accuracy:95.300000 | Dev-accuracy : 91.799995\n",
      "Ep  207 | loss 9.720414 | Train_Accuracy:95.400000 | Dev-accuracy : 92.599998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep  208 | loss 9.303218 | Train_Accuracy:95.800000 | Dev-accuracy : 91.900002\n",
      "Ep  209 | loss 8.498889 | Train_Accuracy:95.900000 | Dev-accuracy : 92.099998\n",
      "Ep  210 | loss 8.640466 | Train_Accuracy:96.200000 | Dev-accuracy : 92.199997\n",
      "Ep  211 | loss 9.683076 | Train_Accuracy:95.300000 | Dev-accuracy : 91.799995\n",
      "Ep  212 | loss 8.185401 | Train_Accuracy:96.400000 | Dev-accuracy : 92.000000\n",
      "Ep  213 | loss 8.972072 | Train_Accuracy:95.800000 | Dev-accuracy : 92.199997\n",
      "Ep  214 | loss 8.647054 | Train_Accuracy:95.500000 | Dev-accuracy : 91.900002\n",
      "Ep  215 | loss 8.286928 | Train_Accuracy:96.100000 | Dev-accuracy : 91.900002\n",
      "Ep  216 | loss 9.752429 | Train_Accuracy:95.300000 | Dev-accuracy : 92.599998\n",
      "Ep  217 | loss 9.629373 | Train_Accuracy:96.000000 | Dev-accuracy : 92.400002\n",
      "Ep  218 | loss 8.706508 | Train_Accuracy:95.700000 | Dev-accuracy : 91.900002\n",
      "Ep  219 | loss 8.698143 | Train_Accuracy:96.300000 | Dev-accuracy : 92.599998\n",
      "Ep  220 | loss 7.427241 | Train_Accuracy:96.900000 | Dev-accuracy : 92.299995\n",
      "Ep  221 | loss 8.256102 | Train_Accuracy:96.100000 | Dev-accuracy : 92.699997\n",
      "Ep  222 | loss 7.696073 | Train_Accuracy:96.400000 | Dev-accuracy : 91.900002\n",
      "Ep  223 | loss 8.119292 | Train_Accuracy:96.500000 | Dev-accuracy : 92.099998\n",
      "Ep  224 | loss 7.236212 | Train_Accuracy:97.200000 | Dev-accuracy : 92.699997\n",
      "Ep  225 | loss 8.998110 | Train_Accuracy:95.600000 | Dev-accuracy : 92.900002\n",
      "Ep  226 | loss 7.980390 | Train_Accuracy:95.900000 | Dev-accuracy : 92.400002\n",
      "Ep  227 | loss 8.271453 | Train_Accuracy:95.700000 | Dev-accuracy : 92.400002\n",
      "Ep  228 | loss 8.087723 | Train_Accuracy:96.100000 | Dev-accuracy : 92.599998\n",
      "Ep  229 | loss 8.444651 | Train_Accuracy:95.700000 | Dev-accuracy : 92.699997\n",
      "Ep  230 | loss 7.959006 | Train_Accuracy:96.100000 | Dev-accuracy : 92.000000\n",
      "Ep  231 | loss 8.364817 | Train_Accuracy:96.000000 | Dev-accuracy : 92.299995\n",
      "Ep  232 | loss 7.690576 | Train_Accuracy:96.100000 | Dev-accuracy : 92.799995\n",
      "Ep  233 | loss 9.374514 | Train_Accuracy:95.400000 | Dev-accuracy : 92.199997\n",
      "Ep  234 | loss 7.448924 | Train_Accuracy:96.600000 | Dev-accuracy : 92.299995\n",
      "Ep  235 | loss 7.916994 | Train_Accuracy:97.300000 | Dev-accuracy : 91.799995\n",
      "Ep  236 | loss 7.805881 | Train_Accuracy:95.900000 | Dev-accuracy : 92.299995\n",
      "Ep  237 | loss 8.125607 | Train_Accuracy:96.300000 | Dev-accuracy : 91.599998\n",
      "Ep  238 | loss 8.485384 | Train_Accuracy:95.800000 | Dev-accuracy : 91.799995\n",
      "Ep  239 | loss 8.075614 | Train_Accuracy:96.400000 | Dev-accuracy : 92.099998\n",
      "Ep  240 | loss 7.479877 | Train_Accuracy:96.200000 | Dev-accuracy : 92.299995\n",
      "Ep  241 | loss 10.802236 | Train_Accuracy:95.200000 | Dev-accuracy : 92.500000\n",
      "Ep  242 | loss 10.346257 | Train_Accuracy:95.700000 | Dev-accuracy : 92.000000\n",
      "Ep  243 | loss 8.085265 | Train_Accuracy:96.800000 | Dev-accuracy : 91.799995\n",
      "Ep  244 | loss 8.168607 | Train_Accuracy:96.400000 | Dev-accuracy : 92.500000\n",
      "Ep  245 | loss 7.877676 | Train_Accuracy:96.600000 | Dev-accuracy : 92.199997\n",
      "Ep  246 | loss 8.745765 | Train_Accuracy:95.300000 | Dev-accuracy : 92.599998\n",
      "Ep  247 | loss 8.284651 | Train_Accuracy:95.900000 | Dev-accuracy : 92.000000\n",
      "Ep  248 | loss 9.092262 | Train_Accuracy:95.900000 | Dev-accuracy : 91.900002\n",
      "Ep  249 | loss 7.641343 | Train_Accuracy:96.600000 | Dev-accuracy : 91.699997\n",
      "Ep  250 | loss 8.218087 | Train_Accuracy:96.100000 | Dev-accuracy : 91.699997\n",
      "Ep  251 | loss 8.166793 | Train_Accuracy:96.500000 | Dev-accuracy : 91.699997\n",
      "Ep  252 | loss 8.284846 | Train_Accuracy:96.400000 | Dev-accuracy : 92.299995\n",
      "Ep  253 | loss 7.096066 | Train_Accuracy:96.900000 | Dev-accuracy : 92.500000\n",
      "Ep  254 | loss 8.214409 | Train_Accuracy:96.500000 | Dev-accuracy : 92.299995\n",
      "Ep  255 | loss 7.240824 | Train_Accuracy:96.400000 | Dev-accuracy : 92.000000\n",
      "Ep  256 | loss 8.122331 | Train_Accuracy:96.000000 | Dev-accuracy : 92.799995\n",
      "Ep  257 | loss 8.960091 | Train_Accuracy:95.100000 | Dev-accuracy : 92.400002\n",
      "Ep  258 | loss 7.749738 | Train_Accuracy:96.900000 | Dev-accuracy : 92.599998\n",
      "Ep  259 | loss 8.550960 | Train_Accuracy:96.100000 | Dev-accuracy : 93.099998\n",
      "Ep  260 | loss 7.477147 | Train_Accuracy:96.700000 | Dev-accuracy : 92.400002\n",
      "Ep  261 | loss 8.563806 | Train_Accuracy:96.200000 | Dev-accuracy : 92.500000\n",
      "Ep  262 | loss 8.218618 | Train_Accuracy:96.800000 | Dev-accuracy : 92.699997\n",
      "Ep  263 | loss 7.431977 | Train_Accuracy:96.200000 | Dev-accuracy : 92.500000\n",
      "Ep  264 | loss 8.719853 | Train_Accuracy:96.600000 | Dev-accuracy : 92.199997\n",
      "Ep  265 | loss 7.851648 | Train_Accuracy:96.300000 | Dev-accuracy : 92.500000\n",
      "Ep  266 | loss 7.993881 | Train_Accuracy:95.500000 | Dev-accuracy : 92.500000\n",
      "Ep  267 | loss 7.947499 | Train_Accuracy:97.200000 | Dev-accuracy : 91.900002\n",
      "Ep  268 | loss 9.350719 | Train_Accuracy:95.600000 | Dev-accuracy : 92.000000\n",
      "Ep  269 | loss 8.209362 | Train_Accuracy:95.900000 | Dev-accuracy : 92.500000\n",
      "Ep  270 | loss 7.855318 | Train_Accuracy:96.500000 | Dev-accuracy : 92.199997\n",
      "Ep  271 | loss 8.159645 | Train_Accuracy:96.500000 | Dev-accuracy : 92.799995\n",
      "Ep  272 | loss 7.967515 | Train_Accuracy:96.500000 | Dev-accuracy : 92.000000\n",
      "Ep  273 | loss 8.878875 | Train_Accuracy:96.200000 | Dev-accuracy : 92.500000\n",
      "Ep  274 | loss 6.972820 | Train_Accuracy:97.100000 | Dev-accuracy : 92.500000\n",
      "Ep  275 | loss 8.510641 | Train_Accuracy:96.500000 | Dev-accuracy : 91.799995\n",
      "Ep  276 | loss 8.266494 | Train_Accuracy:96.100000 | Dev-accuracy : 92.799995\n",
      "Ep  277 | loss 7.990447 | Train_Accuracy:96.200000 | Dev-accuracy : 92.599998\n",
      "Ep  278 | loss 8.587332 | Train_Accuracy:95.700000 | Dev-accuracy : 92.299995\n",
      "Ep  279 | loss 8.655763 | Train_Accuracy:96.000000 | Dev-accuracy : 92.400002\n",
      "Ep  280 | loss 7.620444 | Train_Accuracy:96.300000 | Dev-accuracy : 92.099998\n",
      "Ep  281 | loss 7.688751 | Train_Accuracy:96.800000 | Dev-accuracy : 92.199997\n",
      "Ep  282 | loss 9.148799 | Train_Accuracy:95.800000 | Dev-accuracy : 91.900002\n",
      "Ep  283 | loss 7.699611 | Train_Accuracy:96.200000 | Dev-accuracy : 92.400002\n",
      "Ep  284 | loss 7.453757 | Train_Accuracy:96.600000 | Dev-accuracy : 92.000000\n",
      "Ep  285 | loss 9.010245 | Train_Accuracy:96.100000 | Dev-accuracy : 92.699997\n",
      "Ep  286 | loss 7.956996 | Train_Accuracy:96.000000 | Dev-accuracy : 92.299995\n",
      "Ep  287 | loss 6.255190 | Train_Accuracy:97.300000 | Dev-accuracy : 92.000000\n",
      "Ep  288 | loss 7.915417 | Train_Accuracy:95.700000 | Dev-accuracy : 92.400002\n",
      "Ep  289 | loss 8.540870 | Train_Accuracy:96.000000 | Dev-accuracy : 92.000000\n",
      "Ep  290 | loss 6.306891 | Train_Accuracy:97.200000 | Dev-accuracy : 92.699997\n",
      "Ep  291 | loss 8.401454 | Train_Accuracy:96.300000 | Dev-accuracy : 92.099998\n",
      "Ep  292 | loss 6.838430 | Train_Accuracy:96.700000 | Dev-accuracy : 92.099998\n",
      "Ep  293 | loss 6.873410 | Train_Accuracy:97.500000 | Dev-accuracy : 92.199997\n",
      "Ep  294 | loss 8.712372 | Train_Accuracy:95.500000 | Dev-accuracy : 92.000000\n",
      "Ep  295 | loss 7.139186 | Train_Accuracy:97.200000 | Dev-accuracy : 91.900002\n",
      "Ep  296 | loss 7.097420 | Train_Accuracy:96.100000 | Dev-accuracy : 92.400002\n",
      "Ep  297 | loss 6.952937 | Train_Accuracy:97.000000 | Dev-accuracy : 91.900002\n",
      "Ep  298 | loss 7.478433 | Train_Accuracy:96.700000 | Dev-accuracy : 92.099998\n",
      "Ep  299 | loss 6.734377 | Train_Accuracy:96.900000 | Dev-accuracy : 92.099998\n",
      "Ep  300 | loss 6.176056 | Train_Accuracy:97.900000 | Dev-accuracy : 92.299995\n",
      "Ep  301 | loss 6.601549 | Train_Accuracy:96.700000 | Dev-accuracy : 91.699997\n",
      "Ep  302 | loss 9.083135 | Train_Accuracy:96.000000 | Dev-accuracy : 92.000000\n",
      "Ep  303 | loss 7.572159 | Train_Accuracy:96.200000 | Dev-accuracy : 91.799995\n",
      "Ep  304 | loss 8.323486 | Train_Accuracy:95.800000 | Dev-accuracy : 91.799995\n",
      "Ep  305 | loss 7.771637 | Train_Accuracy:96.400000 | Dev-accuracy : 91.900002\n",
      "Ep  306 | loss 6.865244 | Train_Accuracy:96.900000 | Dev-accuracy : 92.000000\n",
      "Ep  307 | loss 8.026458 | Train_Accuracy:96.200000 | Dev-accuracy : 92.199997\n",
      "Ep  308 | loss 8.271634 | Train_Accuracy:96.500000 | Dev-accuracy : 91.799995\n",
      "Ep  309 | loss 6.955123 | Train_Accuracy:97.000000 | Dev-accuracy : 91.900002\n",
      "Ep  310 | loss 8.139728 | Train_Accuracy:96.000000 | Dev-accuracy : 92.599998\n",
      "Ep  311 | loss 7.149161 | Train_Accuracy:97.000000 | Dev-accuracy : 92.199997\n",
      "Ep  312 | loss 8.871764 | Train_Accuracy:95.700000 | Dev-accuracy : 91.699997\n",
      "Ep  313 | loss 8.528600 | Train_Accuracy:95.200000 | Dev-accuracy : 92.299995\n",
      "Ep  314 | loss 8.769761 | Train_Accuracy:96.000000 | Dev-accuracy : 92.099998\n",
      "Ep  315 | loss 6.729868 | Train_Accuracy:97.900000 | Dev-accuracy : 92.599998\n",
      "Ep  316 | loss 8.502504 | Train_Accuracy:95.900000 | Dev-accuracy : 91.900002\n",
      "Ep  317 | loss 7.596195 | Train_Accuracy:96.700000 | Dev-accuracy : 91.900002\n",
      "Ep  318 | loss 7.105278 | Train_Accuracy:96.700000 | Dev-accuracy : 92.299995\n",
      "Ep  319 | loss 7.253819 | Train_Accuracy:96.000000 | Dev-accuracy : 91.599998\n",
      "Ep  320 | loss 6.952538 | Train_Accuracy:97.100000 | Dev-accuracy : 91.900002\n",
      "Ep  321 | loss 7.039092 | Train_Accuracy:96.600000 | Dev-accuracy : 92.000000\n",
      "Ep  322 | loss 8.495577 | Train_Accuracy:96.300000 | Dev-accuracy : 91.900002\n",
      "Ep  323 | loss 7.171937 | Train_Accuracy:96.500000 | Dev-accuracy : 91.900002\n",
      "Ep  324 | loss 7.452021 | Train_Accuracy:97.000000 | Dev-accuracy : 92.000000\n",
      "Ep  325 | loss 6.219051 | Train_Accuracy:97.500000 | Dev-accuracy : 92.099998\n",
      "Ep  326 | loss 6.471361 | Train_Accuracy:97.700000 | Dev-accuracy : 92.000000\n",
      "Ep  327 | loss 8.006528 | Train_Accuracy:96.500000 | Dev-accuracy : 92.000000\n",
      "Ep  328 | loss 6.244028 | Train_Accuracy:97.400000 | Dev-accuracy : 92.000000\n",
      "Ep  329 | loss 7.990547 | Train_Accuracy:96.100000 | Dev-accuracy : 91.799995\n",
      "Ep  330 | loss 8.306828 | Train_Accuracy:96.000000 | Dev-accuracy : 92.299995\n",
      "Ep  331 | loss 7.162871 | Train_Accuracy:97.000000 | Dev-accuracy : 92.799995\n",
      "Ep  332 | loss 7.299952 | Train_Accuracy:96.800000 | Dev-accuracy : 92.400002\n",
      "Ep  333 | loss 6.383137 | Train_Accuracy:97.400000 | Dev-accuracy : 92.699997\n",
      "Ep  334 | loss 7.277887 | Train_Accuracy:96.000000 | Dev-accuracy : 92.099998\n",
      "Ep  335 | loss 7.708992 | Train_Accuracy:96.600000 | Dev-accuracy : 92.000000\n",
      "Ep  336 | loss 9.162637 | Train_Accuracy:95.100000 | Dev-accuracy : 92.299995\n",
      "Ep  337 | loss 6.598269 | Train_Accuracy:96.900000 | Dev-accuracy : 92.099998\n",
      "Ep  338 | loss 7.394128 | Train_Accuracy:97.100000 | Dev-accuracy : 92.900002\n",
      "Ep  339 | loss 7.137172 | Train_Accuracy:96.900000 | Dev-accuracy : 92.500000\n",
      "Ep  340 | loss 5.903140 | Train_Accuracy:97.400000 | Dev-accuracy : 91.900002\n",
      "Ep  341 | loss 7.177284 | Train_Accuracy:96.900000 | Dev-accuracy : 92.500000\n",
      "Ep  342 | loss 7.579619 | Train_Accuracy:96.200000 | Dev-accuracy : 92.099998\n",
      "Ep  343 | loss 6.889415 | Train_Accuracy:96.600000 | Dev-accuracy : 92.299995\n",
      "Ep  344 | loss 6.854853 | Train_Accuracy:96.800000 | Dev-accuracy : 91.799995\n",
      "Ep  345 | loss 7.582655 | Train_Accuracy:96.500000 | Dev-accuracy : 92.599998\n",
      "Ep  346 | loss 7.215381 | Train_Accuracy:96.900000 | Dev-accuracy : 92.099998\n",
      "Ep  347 | loss 6.715849 | Train_Accuracy:96.900000 | Dev-accuracy : 92.299995\n",
      "Ep  348 | loss 6.500716 | Train_Accuracy:96.900000 | Dev-accuracy : 92.699997\n",
      "Ep  349 | loss 6.631051 | Train_Accuracy:97.500000 | Dev-accuracy : 92.400002\n",
      "Ep  350 | loss 7.049420 | Train_Accuracy:96.300000 | Dev-accuracy : 92.099998\n",
      "Ep  351 | loss 7.192557 | Train_Accuracy:97.000000 | Dev-accuracy : 92.699997\n",
      "Ep  352 | loss 8.921638 | Train_Accuracy:96.000000 | Dev-accuracy : 92.900002\n",
      "Ep  353 | loss 8.409819 | Train_Accuracy:95.800000 | Dev-accuracy : 92.500000\n",
      "Ep  354 | loss 7.541417 | Train_Accuracy:96.500000 | Dev-accuracy : 92.599998\n",
      "Ep  355 | loss 7.081767 | Train_Accuracy:96.900000 | Dev-accuracy : 92.199997\n",
      "Ep  356 | loss 7.420745 | Train_Accuracy:97.000000 | Dev-accuracy : 92.000000\n",
      "Ep  357 | loss 7.142269 | Train_Accuracy:96.800000 | Dev-accuracy : 92.500000\n",
      "Ep  358 | loss 7.213240 | Train_Accuracy:97.300000 | Dev-accuracy : 92.199997\n",
      "Ep  359 | loss 7.391403 | Train_Accuracy:96.400000 | Dev-accuracy : 92.599998\n",
      "Ep  360 | loss 5.770850 | Train_Accuracy:97.900000 | Dev-accuracy : 92.099998\n",
      "Ep  361 | loss 7.327865 | Train_Accuracy:97.400000 | Dev-accuracy : 92.799995\n",
      "Ep  362 | loss 8.099970 | Train_Accuracy:96.600000 | Dev-accuracy : 92.599998\n",
      "Ep  363 | loss 7.174187 | Train_Accuracy:96.900000 | Dev-accuracy : 92.299995\n",
      "Ep  364 | loss 7.564316 | Train_Accuracy:96.700000 | Dev-accuracy : 92.500000\n",
      "Ep  365 | loss 7.479845 | Train_Accuracy:96.700000 | Dev-accuracy : 92.500000\n",
      "Ep  366 | loss 5.529453 | Train_Accuracy:97.100000 | Dev-accuracy : 92.500000\n",
      "Ep  367 | loss 5.415728 | Train_Accuracy:97.700000 | Dev-accuracy : 92.400002\n",
      "Ep  368 | loss 6.789682 | Train_Accuracy:96.700000 | Dev-accuracy : 92.099998\n",
      "Ep  369 | loss 7.321399 | Train_Accuracy:96.200000 | Dev-accuracy : 91.699997\n",
      "Ep  370 | loss 6.561247 | Train_Accuracy:97.000000 | Dev-accuracy : 92.099998\n",
      "Ep  371 | loss 5.014048 | Train_Accuracy:97.900000 | Dev-accuracy : 92.099998\n",
      "Ep  372 | loss 6.984147 | Train_Accuracy:96.600000 | Dev-accuracy : 92.099998\n",
      "Ep  373 | loss 7.656551 | Train_Accuracy:96.500000 | Dev-accuracy : 92.500000\n",
      "Ep  374 | loss 7.647521 | Train_Accuracy:97.200000 | Dev-accuracy : 92.299995\n",
      "Ep  375 | loss 7.326539 | Train_Accuracy:95.500000 | Dev-accuracy : 92.299995\n",
      "Ep  376 | loss 8.190806 | Train_Accuracy:96.500000 | Dev-accuracy : 91.699997\n",
      "Ep  377 | loss 6.214110 | Train_Accuracy:97.100000 | Dev-accuracy : 92.000000\n",
      "Ep  378 | loss 7.619238 | Train_Accuracy:96.400000 | Dev-accuracy : 92.500000\n",
      "Ep  379 | loss 5.747655 | Train_Accuracy:97.600000 | Dev-accuracy : 91.699997\n",
      "Ep  380 | loss 7.599269 | Train_Accuracy:96.400000 | Dev-accuracy : 91.799995\n",
      "Ep  381 | loss 4.920803 | Train_Accuracy:98.300000 | Dev-accuracy : 92.400002\n",
      "Ep  382 | loss 7.948313 | Train_Accuracy:96.300000 | Dev-accuracy : 92.400002\n",
      "Ep  383 | loss 6.355307 | Train_Accuracy:96.900000 | Dev-accuracy : 92.599998\n",
      "Ep  384 | loss 4.889740 | Train_Accuracy:98.400000 | Dev-accuracy : 92.199997\n",
      "Ep  385 | loss 6.319996 | Train_Accuracy:97.300000 | Dev-accuracy : 92.400002\n",
      "Ep  386 | loss 7.058003 | Train_Accuracy:96.400000 | Dev-accuracy : 92.099998\n",
      "Ep  387 | loss 7.433312 | Train_Accuracy:96.700000 | Dev-accuracy : 92.199997\n",
      "Ep  388 | loss 7.950003 | Train_Accuracy:96.000000 | Dev-accuracy : 92.500000\n",
      "Ep  389 | loss 5.655534 | Train_Accuracy:97.000000 | Dev-accuracy : 92.500000\n",
      "Ep  390 | loss 6.597991 | Train_Accuracy:96.900000 | Dev-accuracy : 92.699997\n",
      "Ep  391 | loss 6.820338 | Train_Accuracy:96.900000 | Dev-accuracy : 92.400002\n",
      "Ep  392 | loss 6.658707 | Train_Accuracy:96.900000 | Dev-accuracy : 92.400002\n",
      "Ep  393 | loss 5.838151 | Train_Accuracy:97.700000 | Dev-accuracy : 92.000000\n",
      "Ep  394 | loss 6.262096 | Train_Accuracy:97.400000 | Dev-accuracy : 92.599998\n",
      "Ep  395 | loss 7.624384 | Train_Accuracy:97.100000 | Dev-accuracy : 91.900002\n",
      "Ep  396 | loss 6.000927 | Train_Accuracy:97.300000 | Dev-accuracy : 92.400002\n",
      "Ep  397 | loss 6.066815 | Train_Accuracy:97.600000 | Dev-accuracy : 91.799995\n",
      "Ep  398 | loss 6.571686 | Train_Accuracy:97.100000 | Dev-accuracy : 92.299995\n",
      "Ep  399 | loss 7.105888 | Train_Accuracy:97.000000 | Dev-accuracy : 92.799995\n",
      "Ep  400 | loss 5.939818 | Train_Accuracy:97.300000 | Dev-accuracy : 92.500000\n",
      "Ep  401 | loss 6.803992 | Train_Accuracy:97.100000 | Dev-accuracy : 92.199997\n",
      "Ep  402 | loss 9.489207 | Train_Accuracy:95.600000 | Dev-accuracy : 92.400002\n",
      "Ep  403 | loss 5.933018 | Train_Accuracy:97.500000 | Dev-accuracy : 92.199997\n",
      "Ep  404 | loss 6.900011 | Train_Accuracy:97.200000 | Dev-accuracy : 91.900002\n",
      "Ep  405 | loss 6.150764 | Train_Accuracy:97.400000 | Dev-accuracy : 92.099998\n",
      "Ep  406 | loss 7.219497 | Train_Accuracy:96.600000 | Dev-accuracy : 92.400002\n",
      "Ep  407 | loss 6.037667 | Train_Accuracy:96.600000 | Dev-accuracy : 92.299995\n",
      "Ep  408 | loss 8.149081 | Train_Accuracy:96.400000 | Dev-accuracy : 92.000000\n",
      "Ep  409 | loss 6.752010 | Train_Accuracy:96.900000 | Dev-accuracy : 92.099998\n",
      "Ep  410 | loss 7.411061 | Train_Accuracy:96.600000 | Dev-accuracy : 92.299995\n",
      "Ep  411 | loss 6.978527 | Train_Accuracy:97.100000 | Dev-accuracy : 92.099998\n",
      "Ep  412 | loss 7.504517 | Train_Accuracy:96.500000 | Dev-accuracy : 92.400002\n",
      "Ep  413 | loss 6.425501 | Train_Accuracy:97.300000 | Dev-accuracy : 92.299995\n",
      "Ep  414 | loss 5.686111 | Train_Accuracy:97.500000 | Dev-accuracy : 92.000000\n",
      "Ep  415 | loss 6.054381 | Train_Accuracy:97.000000 | Dev-accuracy : 92.099998\n",
      "Ep  416 | loss 6.763431 | Train_Accuracy:97.000000 | Dev-accuracy : 92.699997\n",
      "Ep  417 | loss 5.985330 | Train_Accuracy:97.500000 | Dev-accuracy : 92.400002\n",
      "Ep  418 | loss 5.429309 | Train_Accuracy:97.800000 | Dev-accuracy : 92.099998\n",
      "Ep  419 | loss 6.530653 | Train_Accuracy:97.600000 | Dev-accuracy : 92.500000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep  420 | loss 7.226664 | Train_Accuracy:97.000000 | Dev-accuracy : 92.900002\n",
      "Ep  421 | loss 4.835949 | Train_Accuracy:98.100000 | Dev-accuracy : 92.799995\n",
      "Ep  422 | loss 5.832998 | Train_Accuracy:97.600000 | Dev-accuracy : 92.500000\n",
      "Ep  423 | loss 6.818042 | Train_Accuracy:96.900000 | Dev-accuracy : 92.799995\n",
      "Ep  424 | loss 6.366802 | Train_Accuracy:97.600000 | Dev-accuracy : 92.400002\n",
      "Ep  425 | loss 6.882642 | Train_Accuracy:97.200000 | Dev-accuracy : 92.699997\n",
      "Ep  426 | loss 7.091166 | Train_Accuracy:97.200000 | Dev-accuracy : 92.299995\n",
      "Ep  427 | loss 7.572747 | Train_Accuracy:96.300000 | Dev-accuracy : 92.599998\n",
      "Ep  428 | loss 6.572835 | Train_Accuracy:96.500000 | Dev-accuracy : 92.599998\n",
      "Ep  429 | loss 5.794234 | Train_Accuracy:97.500000 | Dev-accuracy : 92.099998\n",
      "Ep  430 | loss 5.966709 | Train_Accuracy:97.200000 | Dev-accuracy : 92.299995\n",
      "Ep  431 | loss 6.182104 | Train_Accuracy:97.300000 | Dev-accuracy : 92.199997\n",
      "Ep  432 | loss 6.648570 | Train_Accuracy:96.500000 | Dev-accuracy : 92.299995\n",
      "Ep  433 | loss 5.258291 | Train_Accuracy:97.700000 | Dev-accuracy : 92.299995\n",
      "Ep  434 | loss 5.976453 | Train_Accuracy:97.600000 | Dev-accuracy : 91.900002\n",
      "Ep  435 | loss 4.610481 | Train_Accuracy:98.300000 | Dev-accuracy : 92.599998\n",
      "Ep  436 | loss 6.248719 | Train_Accuracy:96.700000 | Dev-accuracy : 92.500000\n",
      "Ep  437 | loss 5.545919 | Train_Accuracy:97.100000 | Dev-accuracy : 92.500000\n",
      "Ep  438 | loss 6.260166 | Train_Accuracy:97.500000 | Dev-accuracy : 92.299995\n",
      "Ep  439 | loss 6.689339 | Train_Accuracy:97.300000 | Dev-accuracy : 92.500000\n",
      "Ep  440 | loss 6.316789 | Train_Accuracy:97.200000 | Dev-accuracy : 92.699997\n",
      "Ep  441 | loss 6.844159 | Train_Accuracy:97.300000 | Dev-accuracy : 92.299995\n",
      "Ep  442 | loss 5.883184 | Train_Accuracy:97.300000 | Dev-accuracy : 92.299995\n",
      "Ep  443 | loss 5.410524 | Train_Accuracy:97.300000 | Dev-accuracy : 92.400002\n",
      "Ep  444 | loss 6.801351 | Train_Accuracy:96.500000 | Dev-accuracy : 92.400002\n",
      "Ep  445 | loss 7.215163 | Train_Accuracy:96.900000 | Dev-accuracy : 92.199997\n",
      "Ep  446 | loss 5.416116 | Train_Accuracy:97.800000 | Dev-accuracy : 92.599998\n",
      "Ep  447 | loss 7.836110 | Train_Accuracy:96.500000 | Dev-accuracy : 92.900002\n",
      "Ep  448 | loss 5.624356 | Train_Accuracy:97.800000 | Dev-accuracy : 92.500000\n",
      "Ep  449 | loss 6.514739 | Train_Accuracy:96.800000 | Dev-accuracy : 92.199997\n",
      "Ep  450 | loss 5.415644 | Train_Accuracy:97.500000 | Dev-accuracy : 92.199997\n",
      "Ep  451 | loss 6.161679 | Train_Accuracy:97.300000 | Dev-accuracy : 92.199997\n",
      "Ep  452 | loss 6.650429 | Train_Accuracy:97.400000 | Dev-accuracy : 92.500000\n",
      "Ep  453 | loss 6.374252 | Train_Accuracy:97.200000 | Dev-accuracy : 92.199997\n",
      "Ep  454 | loss 5.442189 | Train_Accuracy:97.800000 | Dev-accuracy : 92.500000\n",
      "Ep  455 | loss 6.008204 | Train_Accuracy:97.600000 | Dev-accuracy : 92.599998\n",
      "Ep  456 | loss 5.877766 | Train_Accuracy:97.400000 | Dev-accuracy : 92.299995\n",
      "Ep  457 | loss 5.576282 | Train_Accuracy:97.100000 | Dev-accuracy : 91.900002\n",
      "Ep  458 | loss 5.713041 | Train_Accuracy:97.700000 | Dev-accuracy : 92.299995\n",
      "Ep  459 | loss 5.128410 | Train_Accuracy:97.300000 | Dev-accuracy : 92.199997\n",
      "Ep  460 | loss 6.865383 | Train_Accuracy:96.800000 | Dev-accuracy : 92.099998\n",
      "Ep  461 | loss 6.550071 | Train_Accuracy:97.400000 | Dev-accuracy : 92.000000\n",
      "Ep  462 | loss 7.865813 | Train_Accuracy:96.200000 | Dev-accuracy : 92.199997\n",
      "Ep  463 | loss 5.372133 | Train_Accuracy:97.700000 | Dev-accuracy : 92.099998\n",
      "Ep  464 | loss 6.456175 | Train_Accuracy:97.000000 | Dev-accuracy : 92.000000\n",
      "Ep  465 | loss 7.324741 | Train_Accuracy:97.000000 | Dev-accuracy : 92.099998\n",
      "Ep  466 | loss 6.137374 | Train_Accuracy:97.300000 | Dev-accuracy : 91.599998\n",
      "Ep  467 | loss 6.154814 | Train_Accuracy:97.100000 | Dev-accuracy : 92.299995\n",
      "Ep  468 | loss 5.591940 | Train_Accuracy:97.500000 | Dev-accuracy : 91.799995\n",
      "Ep  469 | loss 4.954098 | Train_Accuracy:98.500000 | Dev-accuracy : 92.000000\n",
      "Ep  470 | loss 4.975569 | Train_Accuracy:98.000000 | Dev-accuracy : 91.900002\n",
      "Ep  471 | loss 5.797455 | Train_Accuracy:97.600000 | Dev-accuracy : 92.299995\n",
      "Ep  472 | loss 5.469334 | Train_Accuracy:98.100000 | Dev-accuracy : 92.199997\n",
      "Ep  473 | loss 5.288472 | Train_Accuracy:97.500000 | Dev-accuracy : 92.500000\n",
      "Ep  474 | loss 5.642864 | Train_Accuracy:97.100000 | Dev-accuracy : 92.500000\n",
      "Ep  475 | loss 6.067403 | Train_Accuracy:97.400000 | Dev-accuracy : 92.099998\n",
      "Ep  476 | loss 5.429662 | Train_Accuracy:97.800000 | Dev-accuracy : 92.099998\n",
      "Ep  477 | loss 6.757200 | Train_Accuracy:96.800000 | Dev-accuracy : 92.299995\n",
      "Ep  478 | loss 6.011579 | Train_Accuracy:97.300000 | Dev-accuracy : 92.400002\n",
      "Ep  479 | loss 5.014428 | Train_Accuracy:98.000000 | Dev-accuracy : 92.500000\n",
      "Ep  480 | loss 5.564961 | Train_Accuracy:97.500000 | Dev-accuracy : 92.000000\n",
      "Ep  481 | loss 5.938979 | Train_Accuracy:97.200000 | Dev-accuracy : 92.199997\n",
      "Ep  482 | loss 5.012919 | Train_Accuracy:98.000000 | Dev-accuracy : 92.199997\n",
      "Ep  483 | loss 6.788798 | Train_Accuracy:97.300000 | Dev-accuracy : 92.199997\n",
      "Ep  484 | loss 4.840213 | Train_Accuracy:98.500000 | Dev-accuracy : 92.599998\n",
      "Ep  485 | loss 5.901771 | Train_Accuracy:97.300000 | Dev-accuracy : 92.199997\n",
      "Ep  486 | loss 5.742301 | Train_Accuracy:97.200000 | Dev-accuracy : 92.299995\n",
      "Ep  487 | loss 5.204510 | Train_Accuracy:97.700000 | Dev-accuracy : 92.099998\n",
      "Ep  488 | loss 6.224064 | Train_Accuracy:96.900000 | Dev-accuracy : 92.799995\n",
      "Ep  489 | loss 6.428251 | Train_Accuracy:97.000000 | Dev-accuracy : 92.500000\n",
      "Ep  490 | loss 5.598911 | Train_Accuracy:97.600000 | Dev-accuracy : 92.400002\n",
      "Ep  491 | loss 5.986380 | Train_Accuracy:97.700000 | Dev-accuracy : 92.900002\n",
      "Ep  492 | loss 6.466961 | Train_Accuracy:97.500000 | Dev-accuracy : 92.699997\n",
      "Ep  493 | loss 6.536189 | Train_Accuracy:97.000000 | Dev-accuracy : 92.500000\n",
      "Ep  494 | loss 5.782747 | Train_Accuracy:97.100000 | Dev-accuracy : 92.400002\n",
      "Ep  495 | loss 6.572826 | Train_Accuracy:96.700000 | Dev-accuracy : 92.500000\n",
      "Ep  496 | loss 6.293334 | Train_Accuracy:97.500000 | Dev-accuracy : 92.400002\n",
      "Ep  497 | loss 4.668770 | Train_Accuracy:97.700000 | Dev-accuracy : 92.299995\n",
      "Ep  498 | loss 5.131694 | Train_Accuracy:97.600000 | Dev-accuracy : 92.199997\n",
      "Ep  499 | loss 5.653558 | Train_Accuracy:97.600000 | Dev-accuracy : 92.299995\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "model = FeedNet(idim=200,nhid=[10], dropout=[0.2])\n",
    "writer = SummaryWriter(\"/Users/cons13411/runs/\")\n",
    "writer.add_graph(model, iter(train_loader).next()[0])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001,\n",
    "                       weight_decay=0.0,\n",
    "                       betas=(0.9, 0.999),\n",
    "                       eps=1e-8,\n",
    "                       amsgrad=False)\n",
    "num_epoch = 500\n",
    "total_instace = len(train_set)\n",
    "best_cor = 0 \n",
    "for epoch in range(num_epoch):\n",
    "    print('Ep {:4d}'.format(epoch), end='')\n",
    "    epoch_loss = 0\n",
    "    runnng_correct = 0\n",
    "    for data in train_loader:\n",
    "        featur, label = data\n",
    "        label = label.long()\n",
    "        label = label.squeeze()\n",
    "        model.zero_grad()\n",
    "        model.train(mode=True)\n",
    "        output = model(featur)\n",
    "        loss = criterion(output, label)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        _,predicted = torch.max(output,1)\n",
    "        runnng_correct += (predicted == label).sum().item()\n",
    "\n",
    "        \n",
    "    print(' | loss {:4f}'.format(epoch_loss), end='')\n",
    "    print(' | Train_Accuracy:{:2f}'.format(100*(runnng_correct/total_instace)), end='')\n",
    "    num_correct, accu = model.evaluate(dev_loader)\n",
    "    writer.add_scalars('Accuracy', {'Train_accuracy':100*(runnng_correct/total_instace), 'Dev_accuracy':accu }, epoch)\n",
    "    if num_correct > best_cor:\n",
    "        best_cor = num_correct\n",
    "        torch.save(model.state_dict(), \"./model.pth\")\n",
    "        best_model = copy.deepcopy(model)   \n",
    "        print(\"| saved\")\n",
    "    else:\n",
    "        print('')\n",
    "\n",
    "\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataset\n",
      "Generating Document Embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec1ce4c50f6c4daf893166aa60714faa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_set = textData('/Users/cons13411/Downloads/RCV!/mldoc/italian.test',\n",
    "                    emb_model, prefix='it_', language='italian',\n",
    "                    dim=200, subwords=False)\n",
    "test_loader = DataLoader(test_set, batch_size=12, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "t ={'a':2,'b':1}\n",
    "q = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "q += [1]\n",
    "q += [2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
